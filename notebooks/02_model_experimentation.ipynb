{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463bd60",
   "metadata": {},
   "source": [
    "# PATH AND BASIC CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the root directory for data\n",
    "ROOT_DATA_DIR = Path(\"/Volumes/Data_Bravo/Google_research_open_contrail\")\n",
    "TRAIN_DATA_DIR = ROOT_DATA_DIR / \"train\"\n",
    "TEST_DATA_DIR = ROOT_DATA_DIR / \"validation\"\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "SEED = 19\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Setting MPS device to accelerate training\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"âœ… MPS device available, using it for training.\")\n",
    "else:\n",
    "    print(\"âŒ MPS device not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a6697",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# æ•°æ®é¢„å¤„ç†å’Œå›¾åƒç”Ÿæˆå·¥å…·å‡½æ•°\n",
    "# ============================\n",
    "\n",
    "# æ¸©åº¦èŒƒå›´å¸¸é‡å®šä¹‰ - ç”¨äºå«æ˜Ÿå›¾åƒæ•°æ®çš„å½’ä¸€åŒ–\n",
    "# è¿™äº›èŒƒå›´åŸºäºå«æ˜Ÿå›¾åƒä¸­å„æ³¢æ®µçš„ç‰©ç†ç‰¹æ€§å’Œç»éªŒå€¼\n",
    "_T11_BOUNDS = (243, 303)  # Band 11 æ¸©åº¦èŒƒå›´ (Kelvin)ï¼Œç”¨äºè“è‰²é€šé“\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)  # Band 14-11 æ¸©åº¦å·®èŒƒå›´ï¼Œç”¨äºç»¿è‰²é€šé“ï¼Œå¸®åŠ©è¯†åˆ«äº‘é¡¶ç‰¹å¾\n",
    "_TDIFF_BOUNDS = (-4, 2)  # Band 15-14 æ¸©åº¦å·®èŒƒå›´ï¼Œç”¨äºçº¢è‰²é€šé“ï¼Œå¸®åŠ©åŒºåˆ†ä¸åŒç±»å‹çš„äº‘å’Œé£æœºå°¾è¿¹\n",
    "\n",
    "def get_band_images(idx: str, parrent_folder: str, band: str) -> np.array:\n",
    "    \"\"\"\n",
    "    ä»æŒ‡å®šè·¯å¾„åŠ è½½å«æ˜Ÿå›¾åƒçš„å•ä¸ªæ³¢æ®µæ•°æ®\n",
    "    \n",
    "    Args:\n",
    "        idx (str): æ ·æœ¬IDï¼Œå¯¹åº”æ–‡ä»¶å¤¹åç§°\n",
    "        parrent_folder (str): çˆ¶çº§æ–‡ä»¶å¤¹åç§° ('train' æˆ– 'validation')\n",
    "        band (str): æ³¢æ®µç¼–å· ('11', '14', '15' ç­‰)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: åŠ è½½çš„æ³¢æ®µæ•°æ®ï¼Œå½¢çŠ¶é€šå¸¸ä¸º (height, width, time_frames)\n",
    "    \"\"\"\n",
    "    return np.load(\n",
    "        os.path.join(ROOT_DATA_DIR, parrent_folder, idx, f'band_{band}.npy')\n",
    "    )\n",
    "\n",
    "def normalize_range(data, bounds):\n",
    "    \"\"\"\n",
    "    å°†æ•°æ®å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´å†…\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼š\n",
    "    1. ä¸åŒæ³¢æ®µçš„æ•°å€¼èŒƒå›´å·®å¼‚å¾ˆå¤§ï¼Œå½’ä¸€åŒ–åä¾¿äºæ¨¡å‹å­¦ä¹ \n",
    "    2. æœ‰åŠ©äºæ¢¯åº¦ä¸‹é™çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦\n",
    "    3. é˜²æ­¢æŸäº›ç‰¹å¾å› æ•°å€¼è¿‡å¤§è€Œä¸»å¯¼æ¨¡å‹å­¦ä¹ è¿‡ç¨‹\n",
    "    4. ä½¿å¾—RGBå›¾åƒçš„æ¯ä¸ªé€šé“éƒ½åœ¨åˆç†çš„æ˜¾ç¤ºèŒƒå›´å†…\n",
    "    \n",
    "    Args:\n",
    "        data: è¾“å…¥æ•°æ®æ•°ç»„\n",
    "        bounds: æ•°æ®çš„ (æœ€å°å€¼, æœ€å¤§å€¼) å…ƒç»„\n",
    "    \n",
    "    Returns:\n",
    "        å½’ä¸€åŒ–åçš„æ•°æ®ï¼ŒèŒƒå›´ [0, 1]\n",
    "    \n",
    "    æ³¨æ„ï¼šè¶…å‡ºboundsèŒƒå›´çš„å€¼ä¼šè¢«æ˜ å°„åˆ° <0 æˆ– >1ï¼Œé€šå¸¸é…åˆ np.clip ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "\n",
    "def get_ash_color_images(\n",
    "    idx: str, parrent_folder: str, get_mask_frame_only=False\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå‡å½©è‰²åˆæˆå›¾åƒç”¨äºé£æœºå°¾è¿¹æ£€æµ‹\n",
    "    \n",
    "    è¿™ä¸ªå‡½æ•°åˆ›å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºæ£€æµ‹é£æœºå°¾è¿¹çš„RGBå‡å½©è‰²å›¾åƒï¼š\n",
    "    - çº¢è‰²é€šé“ (R): Band 15-14 æ¸©åº¦å·®ï¼Œçªå‡ºæ˜¾ç¤ºä¸åŒé«˜åº¦çš„äº‘å±‚å·®å¼‚\n",
    "    - ç»¿è‰²é€šé“ (G): Band 14-11 æ¸©åº¦å·®ï¼Œå¸®åŠ©è¯†åˆ«äº‘é¡¶æ¸©åº¦ç‰¹å¾\n",
    "    - è“è‰²é€šé“ (B): Band 14 ç»å¯¹æ¸©åº¦ï¼Œæä¾›åŸºç¡€æ¸©åº¦ä¿¡æ¯\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆä½¿ç”¨å‡å½©è‰²ï¼š\n",
    "    1. äººçœ¼æ— æ³•ç›´æ¥è§‚å¯Ÿçº¢å¤–æ³¢æ®µï¼Œå‡å½©è‰²å°†ä¸å¯è§ä¿¡æ¯è½¬æ¢ä¸ºå¯è§ä¿¡æ¯\n",
    "    2. æ¸©åº¦å·®å¼‚æ¯”ç»å¯¹æ¸©åº¦æ›´èƒ½çªå‡ºé£æœºå°¾è¿¹çš„ç‰¹å¾\n",
    "    3. å¤šæ³¢æ®µç»„åˆèƒ½å¤Ÿå¢å¼ºç›®æ ‡ä¸èƒŒæ™¯çš„å¯¹æ¯”åº¦\n",
    "    \n",
    "    Args:\n",
    "        idx (str): æ ·æœ¬ID\n",
    "        parrent_folder (str): æ•°æ®æ–‡ä»¶å¤¹ ('train' æˆ– 'validation')\n",
    "        get_mask_frame_only (bool): æ˜¯å¦åªè·å–maskå¯¹åº”çš„æ—¶é—´å¸§ (ç¬¬4å¸§ï¼Œç´¢å¼•ä¸º4)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: RGBå‡å½©è‰²å›¾åƒï¼Œå½¢çŠ¶ä¸º (height, width, 3) æˆ– (height, width, time_frames, 3)\n",
    "                 æ•°å€¼èŒƒå›´ [0, 1]\n",
    "    \"\"\"\n",
    "    # åŠ è½½ä¸‰ä¸ªå…³é”®çš„çº¢å¤–æ³¢æ®µ\n",
    "    band11 = get_band_images(idx, parrent_folder, '11')  # 8.6 Î¼m æ³¢æ®µ\n",
    "    band14 = get_band_images(idx, parrent_folder, '14')  # 11.2 Î¼m æ³¢æ®µ  \n",
    "    band15 = get_band_images(idx, parrent_folder, '15')  # 12.4 Î¼m æ³¢æ®µ\n",
    "\n",
    "    # å¦‚æœåªéœ€è¦maskå¯¹åº”çš„æ—¶é—´å¸§ï¼Œåˆ™æå–ç¬¬4å¸§\n",
    "    if get_mask_frame_only:\n",
    "        band11 = band11[:, :, 4]\n",
    "        band14 = band14[:, :, 4] \n",
    "        band15 = band15[:, :, 4]\n",
    "\n",
    "    # è®¡ç®—å‡å½©è‰²é€šé“\n",
    "    # çº¢è‰²ï¼šBand 15-14 å·®å€¼ï¼Œç”¨äºæ£€æµ‹å†°æ™¶äº‘çš„ç‰¹å¾\n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    # ç»¿è‰²ï¼šBand 14-11 å·®å€¼ï¼Œç”¨äºæ£€æµ‹äº‘é¡¶é«˜åº¦å’Œæ¸©åº¦ç‰¹å¾\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    # è“è‰²ï¼šBand 14 ç»å¯¹æ¸©åº¦ï¼Œæä¾›åŸºç¡€çƒ­è¾å°„ä¿¡æ¯\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    \n",
    "    # å°†ä¸‰ä¸ªé€šé“åˆå¹¶ä¸ºRGBå›¾åƒï¼Œå¹¶é™åˆ¶åœ¨ [0,1] èŒƒå›´å†…\n",
    "    false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    return false_color\n",
    "\n",
    "def get_mask_image(idx: str, parrent_folder: str) -> np.array:\n",
    "    \"\"\"\n",
    "    åŠ è½½äººå·¥æ ‡æ³¨çš„åƒç´ çº§æ©ç \n",
    "    \n",
    "    Args:\n",
    "        idx (str): æ ·æœ¬ID\n",
    "        parrent_folder (str): æ•°æ®æ–‡ä»¶å¤¹\n",
    "    \n",
    "    Returns:\n",
    "        np.array: äºŒå€¼æ©ç ï¼Œ1è¡¨ç¤ºé£æœºå°¾è¿¹åƒç´ ï¼Œ0è¡¨ç¤ºèƒŒæ™¯åƒç´ \n",
    "                 å½¢çŠ¶é€šå¸¸ä¸º (height, width, time_frames)\n",
    "    \"\"\"\n",
    "    return np.load(\n",
    "        os.path.join(ROOT_DATA_DIR, parrent_folder, idx, 'human_pixel_masks.npy')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7dfa6",
   "metadata": {},
   "source": [
    "# MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7550f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# ============================\n",
    "# U-Net è¯­ä¹‰åˆ†å‰²æ¨¡å‹å®šä¹‰\n",
    "# ============================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Netä¸­çš„åŒå·ç§¯å— - U-Netçš„åŸºç¡€æ„å»ºå•å…ƒ\n",
    "    \n",
    "    ç»“æ„ï¼šConv2d -> BatchNorm2d -> ReLU -> Conv2d -> BatchNorm2d -> ReLU\n",
    "    \n",
    "    è®¾è®¡åŸç†ï¼š\n",
    "    1. åŒå·ç§¯å¢åŠ ç½‘ç»œæ·±åº¦ï¼Œæé«˜ç‰¹å¾æå–èƒ½åŠ›\n",
    "    2. BatchNormåŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§\n",
    "    3. ReLUæ¿€æ´»å‡½æ•°è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¢åŠ éçº¿æ€§\n",
    "    4. padding=1ä¿æŒç‰¹å¾å›¾å°ºå¯¸ä¸å˜\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): è¾“å…¥é€šé“æ•°\n",
    "        out_channels (int): è¾“å‡ºé€šé“æ•°\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            # ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼šæå–å±€éƒ¨ç‰¹å¾\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),  # æ‰¹å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›\n",
    "            nn.ReLU(inplace=True),         # éçº¿æ€§æ¿€æ´»\n",
    "            \n",
    "            # ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼šè¿›ä¸€æ­¥ç»†åŒ–ç‰¹å¾\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Netç¼–ç å™¨çš„ä¸‹é‡‡æ ·å—\n",
    "    \n",
    "    ç»“æ„ï¼šMaxPool2d -> DoubleConv\n",
    "    \n",
    "    åŠŸèƒ½ï¼š\n",
    "    1. MaxPool2dï¼šå°†ç‰¹å¾å›¾å°ºå¯¸å‡åŠï¼Œæ‰©å¤§æ„Ÿå—é‡\n",
    "    2. DoubleConvï¼šæå–æ›´é«˜çº§çš„è¯­ä¹‰ç‰¹å¾\n",
    "    \n",
    "    åœ¨ç¼–ç å™¨è·¯å¾„ä¸­ï¼š\n",
    "    - é€æ¸å‡å°ç©ºé—´åˆ†è¾¨ç‡ (H, W)\n",
    "    - é€æ¸å¢åŠ ç‰¹å¾é€šé“æ•° (C)\n",
    "    - æ•è·æ›´å¤§èŒƒå›´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),  # 2x2æœ€å¤§æ± åŒ–ï¼Œå°ºå¯¸å‡åŠ\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Netè§£ç å™¨çš„ä¸Šé‡‡æ ·å—\n",
    "    \n",
    "    åŠŸèƒ½ï¼š\n",
    "    1. ä¸Šé‡‡æ ·ï¼šæ¢å¤ç©ºé—´åˆ†è¾¨ç‡\n",
    "    2. è·³è·ƒè¿æ¥ï¼šèåˆç¼–ç å™¨çš„ä½çº§ç‰¹å¾å’Œè§£ç å™¨çš„é«˜çº§ç‰¹å¾\n",
    "    3. åŒå·ç§¯ï¼šæ•´åˆå¤šå°ºåº¦ç‰¹å¾\n",
    "    \n",
    "    è·³è·ƒè¿æ¥çš„é‡è¦æ€§ï¼š\n",
    "    - ä¿ç•™ç»†èŠ‚ä¿¡æ¯ï¼šç¼–ç å™¨çš„æµ…å±‚ç‰¹å¾åŒ…å«ä¸°å¯Œçš„ç©ºé—´ç»†èŠ‚\n",
    "    - æ¢¯åº¦æµé€šï¼šå¸®åŠ©æ¢¯åº¦æ›´å¥½åœ°åå‘ä¼ æ’­\n",
    "    - å¤šå°ºåº¦èåˆï¼šç»“åˆä¸åŒå±‚æ¬¡çš„ç‰¹å¾è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            # åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·ï¼šè®¡ç®—æ•ˆç‡é«˜ï¼Œå‚æ•°å°‘\n",
    "            self.up = nn.Upsample(\n",
    "                scale_factor=2, mode='bilinear', align_corners=True\n",
    "            )\n",
    "        else:\n",
    "            # è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·ï¼šå¯å­¦ä¹ çš„ä¸Šé‡‡æ ·ï¼Œä½†å‚æ•°æ›´å¤š\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels // 2, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: æ¥è‡ªä¸Šä¸€å±‚è§£ç å™¨çš„ç‰¹å¾ (ä½åˆ†è¾¨ç‡ï¼Œé«˜è¯­ä¹‰)\n",
    "            x2: æ¥è‡ªç¼–ç å™¨è·³è·ƒè¿æ¥çš„ç‰¹å¾ (é«˜åˆ†è¾¨ç‡ï¼Œä½è¯­ä¹‰)\n",
    "        \"\"\"\n",
    "        # ä¸Šé‡‡æ ·x1åˆ°ä¸x2ç›¸åŒçš„ç©ºé—´å°ºå¯¸\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # å¤„ç†å°ºå¯¸ä¸åŒ¹é…é—®é¢˜ï¼ˆç”±äºæ± åŒ–å¯èƒ½å¯¼è‡´çš„å°ºå¯¸ä¸å®Œå…¨åŒ¹é…ï¼‰\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # å¯¹x1è¿›è¡Œpaddingä»¥åŒ¹é…x2çš„å°ºå¯¸\n",
    "        x1 = nn.functional.pad(\n",
    "            x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2]\n",
    "        )\n",
    "\n",
    "        # è·³è·ƒè¿æ¥ï¼šåœ¨é€šé“ç»´åº¦ä¸Šè¿æ¥ä¸¤ä¸ªç‰¹å¾å›¾\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Netè¯­ä¹‰åˆ†å‰²ç½‘ç»œ\n",
    "    \n",
    "    ç½‘ç»œæ¶æ„ï¼š\n",
    "    1. ç¼–ç å™¨è·¯å¾„ (æ”¶ç¼©è·¯å¾„)ï¼š\n",
    "       - é€å±‚ä¸‹é‡‡æ ·ï¼Œæå–å¤šå°ºåº¦ç‰¹å¾\n",
    "       - é€šé“æ•°é€’å¢ï¼š3â†’64â†’128â†’256â†’512â†’512\n",
    "       - ç©ºé—´å°ºå¯¸é€’å‡ï¼š256Ã—256â†’128Ã—128â†’64Ã—64â†’32Ã—32â†’16Ã—16â†’8Ã—8\n",
    "    \n",
    "    2. è§£ç å™¨è·¯å¾„ (æ‰©å¼ è·¯å¾„)ï¼š\n",
    "       - é€å±‚ä¸Šé‡‡æ ·ï¼Œæ¢å¤ç©ºé—´åˆ†è¾¨ç‡\n",
    "       - è·³è·ƒè¿æ¥èåˆç¼–ç å™¨ç‰¹å¾\n",
    "       - é€šé“æ•°é€’å‡ï¼š1024â†’512â†’256â†’128â†’64\n",
    "       - ç©ºé—´å°ºå¯¸é€’å¢ï¼š8Ã—8â†’16Ã—16â†’32Ã—32â†’64Ã—64â†’128Ã—128â†’256Ã—256\n",
    "    \n",
    "    3. è¾“å‡ºå±‚ï¼š\n",
    "       - 1Ã—1å·ç§¯å°†64é€šé“ç‰¹å¾æ˜ å°„åˆ°1é€šé“\n",
    "       - è¾“å‡ºæ¯ä¸ªåƒç´ å±äºé£æœºå°¾è¿¹çš„æ¦‚ç‡logits\n",
    "    \n",
    "    è¾“å…¥ï¼š(batch_size, 24, 256, 256) - 24é€šé“å‡å½©è‰²å›¾åƒåºåˆ—\n",
    "    è¾“å‡ºï¼š(batch_size, 1, 256, 256) - å•é€šé“åˆ†å‰²æ©ç \n",
    "    \n",
    "    é€‚ç”¨äºé£æœºå°¾è¿¹æ£€æµ‹çš„åŸå› ï¼š\n",
    "    1. ä¿æŒç©ºé—´ç»†èŠ‚ï¼šè·³è·ƒè¿æ¥ä¿ç•™äº†åƒç´ çº§ç²¾åº¦\n",
    "    2. å¤šå°ºåº¦ç‰¹å¾ï¼šèƒ½å¤Ÿæ•è·ä¸åŒå°ºåº¦çš„å°¾è¿¹å½¢çŠ¶\n",
    "    3. ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼šç¼–ç å™¨æä¾›è¶³å¤Ÿçš„æ„Ÿå—é‡\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # ç¼–ç å™¨è·¯å¾„ (ä¸‹é‡‡æ ·)\n",
    "        self.inc = DoubleConv(24, 64)    # åˆå§‹å·ç§¯ï¼š24â†’64é€šé“\n",
    "        self.down1 = Down(64, 128)       # ç¬¬1æ¬¡ä¸‹é‡‡æ ·ï¼š64â†’128é€šé“\n",
    "        self.down2 = Down(128, 256)      # ç¬¬2æ¬¡ä¸‹é‡‡æ ·ï¼š128â†’256é€šé“  \n",
    "        self.down3 = Down(256, 512)      # ç¬¬3æ¬¡ä¸‹é‡‡æ ·ï¼š256â†’512é€šé“\n",
    "        self.down4 = Down(512, 512)      # ç¬¬4æ¬¡ä¸‹é‡‡æ ·ï¼š512â†’512é€šé“ (ç“¶é¢ˆå±‚)\n",
    "        \n",
    "        # è§£ç å™¨è·¯å¾„ (ä¸Šé‡‡æ ·)\n",
    "        self.up1 = Up(1024, 256)         # ç¬¬1æ¬¡ä¸Šé‡‡æ ·ï¼š1024â†’256é€šé“ (512+512è·³è·ƒè¿æ¥)\n",
    "        self.up2 = Up(512, 128)          # ç¬¬2æ¬¡ä¸Šé‡‡æ ·ï¼š512â†’128é€šé“ (256+256è·³è·ƒè¿æ¥)\n",
    "        self.up3 = Up(256, 64)           # ç¬¬3æ¬¡ä¸Šé‡‡æ ·ï¼š256â†’64é€šé“ (128+128è·³è·ƒè¿æ¥)\n",
    "        self.up4 = Up(128, 64)           # ç¬¬4æ¬¡ä¸Šé‡‡æ ·ï¼š128â†’64é€šé“ (64+64è·³è·ƒè¿æ¥)\n",
    "        \n",
    "        # è¾“å‡ºå±‚ï¼šå°†64é€šé“ç‰¹å¾æ˜ å°„ä¸º1é€šé“åˆ†å‰²ç»“æœ\n",
    "        self.outc = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è¿‡ç¨‹\n",
    "        \n",
    "        Args:\n",
    "            x: è¾“å…¥å¼ é‡ (batch_size, 24, 256, 256)\n",
    "            \n",
    "        Returns:\n",
    "            è¾“å‡ºå¼ é‡ (batch_size, 1, 256, 256) - åˆ†å‰²logits\n",
    "        \"\"\"\n",
    "        # ç¼–ç å™¨è·¯å¾„ï¼šé€å±‚æå–ç‰¹å¾å¹¶ä¿å­˜è·³è·ƒè¿æ¥\n",
    "        x1 = self.inc(x)      # [B, 64, 256, 256]\n",
    "        x2 = self.down1(x1)   # [B, 128, 128, 128]  \n",
    "        x3 = self.down2(x2)   # [B, 256, 64, 64]\n",
    "        x4 = self.down3(x3)   # [B, 512, 32, 32]\n",
    "        x5 = self.down4(x4)   # [B, 512, 16, 16]\n",
    "        \n",
    "        # è§£ç å™¨è·¯å¾„ï¼šé€å±‚ä¸Šé‡‡æ ·å¹¶èåˆè·³è·ƒè¿æ¥\n",
    "        x = self.up1(x5, x4)  # [B, 256, 32, 32]\n",
    "        x = self.up2(x, x3)   # [B, 128, 64, 64]\n",
    "        x = self.up3(x, x2)   # [B, 64, 128, 128]  \n",
    "        x = self.up4(x, x1)   # [B, 64, 256, 256]\n",
    "        \n",
    "        # è¾“å‡ºå±‚ï¼šç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²æ©ç \n",
    "        x = self.outc(x)      # [B, 1, 256, 256]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹å¹¶å°†å…¶ç§»åŠ¨åˆ°MPSè®¾å¤‡\n",
    "model = UNet()\n",
    "\n",
    "summary(model, input_size=(24, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c081e6b",
   "metadata": {},
   "source": [
    "# TRAINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de46f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# æŸå¤±å‡½æ•°å®šä¹‰\n",
    "# ============================\n",
    "\n",
    "class Dice(nn.Module):\n",
    "    \"\"\"\n",
    "    Diceç³»æ•°ï¼ˆSÃ¸rensen-Diceç³»æ•°ï¼‰- è¯­ä¹‰åˆ†å‰²ä¸­çš„é‡è¦è¯„ä¼°æŒ‡æ ‡\n",
    "    \n",
    "    Diceç³»æ•°å…¬å¼ï¼šDice = 2Ã—|Aâˆ©B| / (|A| + |B|)\n",
    "    å…¶ä¸­ï¼š\n",
    "    - A: é¢„æµ‹çš„æ­£ä¾‹åƒç´ é›†åˆ\n",
    "    - B: çœŸå®çš„æ­£ä¾‹åƒç´ é›†åˆ  \n",
    "    - |Aâˆ©B|: é¢„æµ‹å’ŒçœŸå®éƒ½ä¸ºæ­£ä¾‹çš„åƒç´ æ•°é‡ï¼ˆçœŸæ­£ä¾‹ï¼‰\n",
    "    - |A|: é¢„æµ‹ä¸ºæ­£ä¾‹çš„åƒç´ æ€»æ•°\n",
    "    - |B|: çœŸå®æ­£ä¾‹çš„åƒç´ æ€»æ•°\n",
    "    \n",
    "    ä¸ºä»€ä¹ˆä½¿ç”¨Diceç³»æ•°ï¼š\n",
    "    1. ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ï¼šé£æœºå°¾è¿¹åƒç´ é€šå¸¸åªå å›¾åƒçš„å¾ˆå°éƒ¨åˆ†ï¼ŒDiceç³»æ•°å¯¹æ­£ä¾‹æ›´æ•æ„Ÿ\n",
    "    2. é‡å åº¦é‡ï¼šç›´æ¥è¡¡é‡é¢„æµ‹åŒºåŸŸä¸çœŸå®åŒºåŸŸçš„é‡å ç¨‹åº¦ï¼Œç¬¦åˆåˆ†å‰²ä»»åŠ¡çš„è¯„ä¼°éœ€æ±‚\n",
    "    3. å¯å¾®åˆ†ï¼šå¯ä»¥ä½œä¸ºæŸå¤±å‡½æ•°è¿›è¡Œåå‘ä¼ æ’­è®­ç»ƒ\n",
    "    4. èŒƒå›´ [0,1]ï¼š1è¡¨ç¤ºå®Œç¾é‡å ï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸é‡å \n",
    "    \n",
    "    ä¸å…¶ä»–æŒ‡æ ‡çš„æ¯”è¾ƒï¼š\n",
    "    - å‡†ç¡®ç‡ (Accuracy): åœ¨æä¸å¹³è¡¡æ•°æ®ä¸­å®¹æ˜“è¢«èƒŒæ™¯åƒç´ ä¸»å¯¼\n",
    "    - IoU (Intersection over Union): ä¸Diceç›¸å…³ï¼Œä½†Diceå¯¹å°ç›®æ ‡æ›´å‹å¥½\n",
    "    - äº¤å‰ç†µæŸå¤±: é€åƒç´ è®¡ç®—ï¼Œä¸è€ƒè™‘åŒºåŸŸçš„è¿é€šæ€§\n",
    "    \n",
    "    Args:\n",
    "        use_sigmoid (bool): æ˜¯å¦å¯¹è¾“å…¥åº”ç”¨sigmoidæ¿€æ´»\n",
    "                           True: è¾“å…¥ä¸ºlogitsï¼Œéœ€è¦è½¬æ¢ä¸ºæ¦‚ç‡\n",
    "                           False: è¾“å…¥å·²ç»æ˜¯æ¦‚ç‡å€¼\n",
    "    \"\"\"\n",
    "    def __init__(self, use_sigmoid=True):\n",
    "        super(Dice, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \"\"\"\n",
    "        è®¡ç®—Diceç³»æ•°\n",
    "        \n",
    "        Args:\n",
    "            inputs: æ¨¡å‹é¢„æµ‹è¾“å‡º (batch_size, 1, H, W)\n",
    "                   å¦‚æœuse_sigmoid=Trueï¼Œåˆ™ä¸ºlogitsï¼›å¦åˆ™ä¸ºæ¦‚ç‡\n",
    "            targets: çœŸå®æ ‡ç­¾ (batch_size, 1, H, W)ï¼ŒäºŒå€¼æ©ç ï¼Œ0æˆ–1\n",
    "            smooth: å¹³æ»‘å¸¸æ•°ï¼Œé˜²æ­¢åˆ†æ¯ä¸º0ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§\n",
    "                   è¾ƒå°çš„smoothå€¼ä½¿å¾—Diceå¯¹å°ç›®æ ‡æ›´æ•æ„Ÿ\n",
    "        \n",
    "        Returns:\n",
    "            dice: Diceç³»æ•°ï¼ŒèŒƒå›´ [0, 1]\n",
    "        \n",
    "        è®¡ç®—ç»†èŠ‚ï¼š\n",
    "        1. å°†å¤šç»´å¼ é‡å±•å¹³ä¸ºä¸€ç»´ï¼Œä¾¿äºè®¡ç®—äº¤é›†å’Œå¹¶é›†\n",
    "        2. è®¡ç®—intersectionï¼šé¢„æµ‹å’ŒçœŸå®éƒ½ä¸ºæ­£çš„åƒç´ æ•°é‡\n",
    "        3. åº”ç”¨Diceå…¬å¼ï¼ŒåŠ å…¥å¹³æ»‘é¡¹é¿å…é™¤é›¶é”™è¯¯\n",
    "        \"\"\"\n",
    "        # å¦‚æœè¾“å…¥æ˜¯logitsï¼Œè½¬æ¢ä¸ºæ¦‚ç‡\n",
    "        if self.use_sigmoid:\n",
    "            inputs = self.sigmoid(inputs)\n",
    "\n",
    "        # å°†å¼ é‡å±•å¹³ä¸ºä¸€ç»´å‘é‡ï¼Œä¾¿äºè®¡ç®—åƒç´ çº§çš„äº¤é›†\n",
    "        # åŸå§‹å½¢çŠ¶ï¼š(batch_size, 1, H, W) -> å±•å¹³åï¼š(batch_size * H * W)\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # è®¡ç®—äº¤é›†ï¼šé¢„æµ‹ä¸ºæ­£ä¸”çœŸå®ä¸ºæ­£çš„åƒç´ æ•°é‡\n",
    "        # inputs * targets: é€å…ƒç´ ç›¸ä¹˜ï¼Œåªæœ‰éƒ½ä¸º1æ—¶ç»“æœæ‰ä¸º1\n",
    "        intersection = (inputs * targets).sum()\n",
    "        \n",
    "        # åº”ç”¨Diceå…¬å¼\n",
    "        # åˆ†å­ï¼š2 * äº¤é›† + å¹³æ»‘é¡¹\n",
    "        # åˆ†æ¯ï¼šé¢„æµ‹æ­£ä¾‹æ€»æ•° + çœŸå®æ­£ä¾‹æ€»æ•° + å¹³æ»‘é¡¹\n",
    "        dice = (2.0 * intersection + smooth) / (\n",
    "            inputs.sum() + targets.sum() + smooth\n",
    "        )\n",
    "\n",
    "        return dice\n",
    "\n",
    "\n",
    "# å®ä¾‹åŒ–Diceè¯„ä¼°å™¨\n",
    "# æ³¨æ„ï¼šè¿™é‡Œåˆ›å»ºçš„æ˜¯è¯„ä¼°æŒ‡æ ‡ï¼Œä¸æ˜¯æŸå¤±å‡½æ•°\n",
    "# è®­ç»ƒæ—¶é€šå¸¸ä½¿ç”¨ 1 - Dice ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæˆ–ä½¿ç”¨ä¸“é—¨çš„DiceLoss\n",
    "dice = Dice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.amp import autocast\n",
    "\n",
    "torch.set_autocast_enabled(True)\n",
    "\n",
    "# ============================\n",
    "# æ¨¡å‹è®­ç»ƒå™¨ç±»\n",
    "# ============================\n",
    "\n",
    "\n",
    "class MyTrainer:\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰è®­ç»ƒå™¨ç±» - å°è£…æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹\n",
    "\n",
    "    åŠŸèƒ½ç‰¹æ€§ï¼š\n",
    "    1. è®­ç»ƒå¾ªç¯ç®¡ç†ï¼šè‡ªåŠ¨åŒ–epochå’Œbatchçš„è¿­ä»£\n",
    "    2. æŸå¤±è®°å½•ï¼šè·Ÿè¸ªè®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å˜åŒ–è¶‹åŠ¿\n",
    "    3. å­¦ä¹ ç‡è°ƒåº¦ï¼šæ”¯æŒåŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´\n",
    "    4. æ¨¡å‹æ£€æŸ¥ç‚¹ï¼šå®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€\n",
    "    5. éªŒè¯è¯„ä¼°ï¼šå®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
    "    6. [æ–°å¢] æ··åˆç²¾åº¦è®­ç»ƒï¼šé€šè¿‡ torch.amp æ”¯æŒ AMPï¼ŒåŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨\n",
    "\n",
    "    è®¾è®¡æ¨¡å¼ï¼š\n",
    "    - å°†è®­ç»ƒé€»è¾‘ä¸æ¨¡å‹å®šä¹‰åˆ†ç¦»ï¼Œæé«˜ä»£ç å¯ç»´æŠ¤æ€§\n",
    "    - æ”¯æŒä¸åŒçš„ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    - è‡ªåŠ¨å¤„ç†GPU/CPUè®¾å¤‡åˆ‡æ¢\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        lr_scheduler,\n",
    "        device,\n",
    "        use_amp: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è®­ç»ƒå™¨\n",
    "\n",
    "        Args:\n",
    "            model: å¾…è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "            optimizer: ä¼˜åŒ–å™¨ (å¦‚Adam, SGDç­‰)\n",
    "            loss_fn: æŸå¤±å‡½æ•° (å¦‚BCEWithLogitsLoss, CrossEntropyLossç­‰)\n",
    "            lr_scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¦‚ExponentialLR, StepLRç­‰)\n",
    "            device: è®­ç»ƒè®¾å¤‡ (ä¾‹å¦‚, torch.device('mps'))\n",
    "            use_amp (bool): æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)\n",
    "        \"\"\"\n",
    "        # è®­ç»ƒå†å²è®°å½•\n",
    "        self.validation_losses = []  # éªŒè¯é›†æŸå¤±å†å²\n",
    "        self.batch_losses = []  # æ‰€æœ‰batchæŸå¤±å†å²\n",
    "        self.epoch_losses = []  # æ¯ä¸ªepochå¹³å‡æŸå¤±\n",
    "        self.learning_rates = []  # å­¦ä¹ ç‡å˜åŒ–å†å²\n",
    "\n",
    "        # è®­ç»ƒç»„ä»¶\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "        # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # self.scaler = torch.amp.GradScaler('mps', enabled=self.use_amp)\n",
    "\n",
    "        print(\n",
    "            f\"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆã€‚è®¾å¤‡: {self.device}, è‡ªåŠ¨æ··åˆç²¾åº¦(AMP): {'å¯ç”¨' if self.use_amp else 'ç¦ç”¨'}\"\n",
    "        )\n",
    "\n",
    "        # éªŒè¯ä¼˜åŒ–å™¨å’Œæ¨¡å‹çš„å‚æ•°æ˜¯å¦åŒ¹é…\n",
    "        self._check_optim_net_aligned()\n",
    "\n",
    "    def _check_optim_net_aligned(self):\n",
    "        \"\"\"\n",
    "        éªŒè¯ä¼˜åŒ–å™¨æ˜¯å¦æ­£ç¡®ç»‘å®šåˆ°æ¨¡å‹å‚æ•°\n",
    "        \"\"\"\n",
    "        assert self.optimizer.param_groups[0]['params'] == list(\n",
    "            self.model.parameters()\n",
    "        ), \"ä¼˜åŒ–å™¨å‚æ•°ä¸æ¨¡å‹å‚æ•°ä¸åŒ¹é…ï¼\"\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_dataloader: DataLoader,\n",
    "        test_dataloader: DataLoader,\n",
    "        epochs: int = 10,\n",
    "        eval_every: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        æ¨¡å‹è®­ç»ƒä¸»å¾ªç¯\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            print(f\"=== Epoch {e+1}/{epochs} ===\")\n",
    "            print(f\"å½“å‰å­¦ä¹ ç‡: {self.lr_scheduler.get_last_lr()}\")\n",
    "            self.learning_rates.append(self.lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            batch_losses, sub_batch_losses = [], []\n",
    "\n",
    "            # ========================\n",
    "            # è®­ç»ƒé˜¶æ®µ (Training Phase)\n",
    "            # ========================\n",
    "            self.model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "\n",
    "                # (ä¿®æ­£) å°†è¿›åº¦æ˜¾ç¤ºé€»è¾‘è°ƒæ•´ä¸º (i+1)ï¼Œé¿å…è·³è¿‡ç¬¬0ä¸ªbatch\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    avg_loss = (\n",
    "                        torch.Tensor(sub_batch_losses).mean()\n",
    "                        if sub_batch_losses\n",
    "                        else 0\n",
    "                    )\n",
    "                    print(\n",
    "                        f' è®­ç»ƒ Batch {i+1:4d}/{len(train_dataloader)} | å¹³å‡æŸå¤±: {avg_loss:.6f}'\n",
    "                    )\n",
    "                    sub_batch_losses.clear()\n",
    "\n",
    "                images, mask = data\n",
    "                images = images.to(self.device)\n",
    "                mask = mask.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # === AMP å‰å‘ä¼ æ’­ (ä¿®æ”¹) ===\n",
    "                # autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šè‡ªåŠ¨ä¸ºä¸åŒopé€‰æ‹©åˆé€‚çš„ç²¾åº¦(float16æˆ–float32)ã€‚\n",
    "                # `device_type` åº”ä¸ä½ çš„è®¾å¤‡åŒ¹é… ('mps', 'cuda', 'cpu')ã€‚\n",
    "                # å‚è€ƒæ–‡æ¡£: https://pytorch.org/docs/stable/amp.html#autocasting\n",
    "                with torch.autocast(\n",
    "                    device_type='mps',\n",
    "                    dtype=torch.bfloat16,\n",
    "                    enabled=self.use_amp,\n",
    "                ):\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.loss_fn(outputs, mask)\n",
    "\n",
    "                # self.scaler.scale(loss).backward()\n",
    "                # self.scaler.step(self.optimizer)\n",
    "                # self.scaler.update()\n",
    "                \n",
    "                # åå‘ä¼ æ’­\n",
    "                loss.backward()\n",
    "                # ä¼˜åŒ–å™¨å¾€åä¼ æ’­æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                loss_item = loss.item()\n",
    "                self.batch_losses.append(loss_item)\n",
    "                batch_losses.append(loss_item)\n",
    "                sub_batch_losses.append(loss_item)\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            mean_epoch_loss = torch.Tensor(batch_losses).mean().item()\n",
    "            self.epoch_losses.append(mean_epoch_loss)\n",
    "            print(f'  è®­ç»ƒæŸå¤±: {mean_epoch_loss:.6f}')\n",
    "\n",
    "            # ========================\n",
    "            # éªŒè¯é˜¶æ®µ (Validation Phase)\n",
    "            # ========================\n",
    "            if (e + 1) % eval_every == 0:\n",
    "                os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "                checkpoint_path = f\"checkpoints/model_checkpoint_e{e+1}.pt\"\n",
    "                torch.save(self.model.state_dict(), checkpoint_path)\n",
    "                print(f\"  æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "                    validation_losses = []\n",
    "                    for i, data in enumerate(test_dataloader):\n",
    "                        images, mask = data\n",
    "                        images = images.to(self.device)\n",
    "                        mask = mask.to(self.device)\n",
    "\n",
    "                        # === AMP éªŒè¯å‰å‘ä¼ æ’­ (ä¿®æ”¹) ===\n",
    "                        # åœ¨éªŒè¯é˜¶æ®µä¹Ÿä½¿ç”¨ autocastï¼Œä»¥è·å¾—æ€§èƒ½æå‡å¹¶ç¡®ä¿ä¸è®­ç»ƒæ—¶è¡Œä¸ºä¸€è‡´ã€‚\n",
    "                        with autocast(\n",
    "                            device_type=self.device.type,\n",
    "                            dtype=torch.float16,\n",
    "                            enabled=self.use_amp,\n",
    "                        ):\n",
    "                            output = self.model(images)\n",
    "                            loss = self.loss_fn(output, mask)\n",
    "\n",
    "                        validation_losses.append(loss.item())\n",
    "\n",
    "                    avg_val_loss = torch.Tensor(validation_losses).mean().item()\n",
    "                    self.validation_losses.append(avg_val_loss)\n",
    "                    print(f\"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}\")\n",
    "\n",
    "                    if len(self.validation_losses) > 1:\n",
    "                        if (\n",
    "                            self.validation_losses[-1]\n",
    "                            < self.validation_losses[-2]\n",
    "                        ):\n",
    "                            print(\"  âœ… éªŒè¯æŸå¤±ä¸‹é™\")\n",
    "                        else:\n",
    "                            print(\"  âš ï¸ éªŒè¯æŸå¤±ä¸Šå‡\")\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f57991",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# è‡ªå®šä¹‰æ•°æ®é›†ç±»\n",
    "# ============================\n",
    "\n",
    "class ContrailsAshDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    é£æœºå°¾è¿¹æ£€æµ‹æ•°æ®é›†ç±» - ç»§æ‰¿è‡ªPyTorch DatasetåŸºç±»\n",
    "    \n",
    "    æ•°æ®é›†ç»“æ„ï¼š\n",
    "    - æ¯ä¸ªæ ·æœ¬åŒ…å«å¤šä¸ªæ—¶é—´å¸§çš„å«æ˜Ÿå›¾åƒæ•°æ®\n",
    "    - è¾“å…¥ï¼š24é€šé“å‡å½©è‰²å›¾åƒåºåˆ— (8ä¸ªæ—¶é—´å¸§ Ã— 3ä¸ªRGBé€šé“)\n",
    "    - è¾“å‡ºï¼šåƒç´ çº§äºŒå€¼æ©ç  (8ä¸ªæ—¶é—´å¸§çš„æ ‡æ³¨)\n",
    "    \n",
    "    æ•°æ®é¢„å¤„ç†æµç¨‹ï¼š\n",
    "    1. åŠ è½½å¤šæ³¢æ®µå«æ˜Ÿå›¾åƒ (band_11, band_14, band_15)\n",
    "    2. è®¡ç®—å‡å½©è‰²åˆæˆå›¾åƒ\n",
    "    3. åŠ è½½äººå·¥æ ‡æ³¨çš„åƒç´ æ©ç \n",
    "    4. å¼ é‡è½¬æ¢å’Œç»´åº¦è°ƒæ•´\n",
    "    5. æ•°æ®ç±»å‹è½¬æ¢ (float32)\n",
    "    \n",
    "    è®¾è®¡è€ƒè™‘ï¼š\n",
    "    - å»¶è¿ŸåŠ è½½ï¼šåªåœ¨éœ€è¦æ—¶æ‰ä»ç£ç›˜è¯»å–æ•°æ®ï¼ŒèŠ‚çœå†…å­˜\n",
    "    - çµæ´»çš„æ•°æ®å˜æ¢ï¼šæ”¯æŒè®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º\n",
    "    - æ‰¹å¤„ç†å‹å¥½ï¼šè¾“å‡ºæ ¼å¼é€‚åˆDataLoaderæ‰¹å¤„ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parrent_folder: str):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ•°æ®é›†\n",
    "        \n",
    "        Args:\n",
    "            parrent_folder (str): æ•°æ®æ–‡ä»¶å¤¹åç§°ï¼Œ'train' æˆ– 'validation'\n",
    "                                 å¯¹åº” ROOT_DATA_DIR/train æˆ– ROOT_DATA_DIR/validation\n",
    "        \n",
    "        æ•°æ®é›†ç»„ç»‡ç»“æ„ï¼š\n",
    "        ROOT_DATA_DIR/\n",
    "        â”œâ”€â”€ train/\n",
    "        â”‚   â”œâ”€â”€ sample_001/\n",
    "        â”‚   â”‚   â”œâ”€â”€ band_11.npy    # 8.6Î¼m çº¢å¤–æ³¢æ®µæ•°æ®\n",
    "        â”‚   â”‚   â”œâ”€â”€ band_14.npy    # 11.2Î¼m çº¢å¤–æ³¢æ®µæ•°æ®  \n",
    "        â”‚   â”‚   â”œâ”€â”€ band_15.npy    # 12.4Î¼m çº¢å¤–æ³¢æ®µæ•°æ®\n",
    "        â”‚   â”‚   â””â”€â”€ human_pixel_masks.npy  # äººå·¥æ ‡æ³¨æ©ç \n",
    "        â”‚   â””â”€â”€ sample_002/\n",
    "        â”‚       â””â”€â”€ ...\n",
    "        â””â”€â”€ validation/\n",
    "            â””â”€â”€ ...\n",
    "        \"\"\"\n",
    "        # è·å–æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        data_folder_path = os.path.join(ROOT_DATA_DIR, parrent_folder)\n",
    "        \n",
    "        # è·å–æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹\n",
    "        all_items = os.listdir(data_folder_path)\n",
    "        \n",
    "        # åªä¿ç•™æ–‡ä»¶å¤¹ï¼Œè¿‡æ»¤æ‰æ–‡ä»¶\n",
    "        sample_ids = [\n",
    "            item for item in all_items \n",
    "            if os.path.isdir(os.path.join(data_folder_path, item))\n",
    "        ]\n",
    "        \n",
    "        # åˆ›å»ºDataFrameå­˜å‚¨æ ·æœ¬ç´¢å¼•ï¼Œä¾¿äºåç»­æŸ¥æ‰¾å’Œç®¡ç†\n",
    "        self.df_idx: pd.DataFrame = pd.DataFrame({'idx': sample_ids})\n",
    "        self.parrent_folder: str = parrent_folder\n",
    "        \n",
    "        print(f\"ğŸ“Š {parrent_folder} æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:\")\n",
    "        print(f\"   æ ·æœ¬æ•°é‡: {len(sample_ids)}\")\n",
    "        print(f\"   æ•°æ®è·¯å¾„: {data_folder_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        è¿”å›æ•°æ®é›†å¤§å°\n",
    "        \n",
    "        PyTorch Datasetå¿…é¡»å®ç°çš„æ–¹æ³•ä¹‹ä¸€\n",
    "        ç”¨äºDataLoaderç¡®å®šæ•°æ®é›†çš„æ€»æ ·æœ¬æ•°\n",
    "        \"\"\"\n",
    "        return len(self.df_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        æ ¹æ®ç´¢å¼•è·å–å•ä¸ªæ ·æœ¬\n",
    "        \n",
    "        PyTorch Datasetå¿…é¡»å®ç°çš„æ–¹æ³•ä¹‹ä¸€\n",
    "        æ”¯æŒç´¢å¼•è®¿é—®: dataset[0], dataset[1], ...\n",
    "        \n",
    "        Args:\n",
    "            idx (int): æ ·æœ¬ç´¢å¼•ï¼ŒèŒƒå›´ [0, len(dataset)-1]\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (images, mask)\n",
    "                - images: è¾“å…¥å›¾åƒå¼ é‡ (24, 256, 256) \n",
    "                         24é€šé“ = 8ä¸ªæ—¶é—´å¸§ Ã— 3ä¸ªRGBé€šé“\n",
    "                - mask: æ ‡ç­¾æ©ç å¼ é‡ (8, 256, 256)\n",
    "                       æ¯ä¸ªæ—¶é—´å¸§å¯¹åº”ä¸€ä¸ªäºŒå€¼æ©ç \n",
    "        \n",
    "        æ•°æ®å˜æ¢è¿‡ç¨‹ï¼š\n",
    "        1. è·å–æ ·æœ¬ID\n",
    "        2. åŠ è½½å‡å½©è‰²å›¾åƒ (256, 256, 8, 3)\n",
    "        3. é‡å¡‘ä¸º (256, 256, 24) - å°†æ—¶é—´å’Œé€šé“ç»´åº¦åˆå¹¶\n",
    "        4. è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è°ƒæ•´ç»´åº¦é¡ºåºä¸º (C, H, W)\n",
    "        5. åŠ è½½æ©ç æ•°æ®å¹¶è¿›è¡Œç›¸åŒçš„å¼ é‡è½¬æ¢\n",
    "        \"\"\"\n",
    "        # æ ¹æ®ç´¢å¼•è·å–æ ·æœ¬ID\n",
    "        image_id: str = str(self.df_idx.iloc[idx]['idx'])\n",
    "        \n",
    "        # åŠ è½½å‡å½©è‰²å›¾åƒæ•°æ®\n",
    "        # get_mask_frame_only=False: åŠ è½½æ‰€æœ‰8ä¸ªæ—¶é—´å¸§\n",
    "        # è¿”å›å½¢çŠ¶: (height=256, width=256, time_frames=8, rgb_channels=3)\n",
    "        ash_color_images = get_ash_color_images(\n",
    "            image_id, self.parrent_folder, get_mask_frame_only=False\n",
    "        )\n",
    "        \n",
    "        # é‡å¡‘å›¾åƒæ•°æ®ï¼šå°†æ—¶é—´å¸§å’ŒRGBé€šé“åˆå¹¶\n",
    "        # (256, 256, 8, 3) -> (256, 256, 24)\n",
    "        # è¿™æ ·åšçš„åŸå› ï¼š\n",
    "        # 1. U-NetæœŸæœ›å›ºå®šçš„è¾“å…¥é€šé“æ•°\n",
    "        # 2. å°†æ—¶åºä¿¡æ¯ç¼–ç ä¸ºå¤šé€šé“ç‰¹å¾\n",
    "        # 3. ç®€åŒ–ç½‘ç»œç»“æ„ï¼Œé¿å…å¤„ç†æ—¶åºç»´åº¦\n",
    "        reshaped_images = np.reshape(ash_color_images, (256, 256, 24))\n",
    "        \n",
    "        # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è°ƒæ•´ç»´åº¦\n",
    "        # numpy: (H, W, C) -> torch: (C, H, W)\n",
    "        # åŸå› ï¼šPyTorchå·ç§¯å±‚æœŸæœ›é€šé“ä¼˜å…ˆçš„æ ¼å¼\n",
    "        images = (\n",
    "            torch.tensor(reshaped_images)\n",
    "            .to(torch.float32)           # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32\n",
    "            .permute(2, 0, 1)           # (H, W, C) -> (C, H, W)\n",
    "        )\n",
    "        \n",
    "        # åŠ è½½æ ‡ç­¾æ©ç \n",
    "        mask_data = get_mask_image(image_id, self.parrent_folder)\n",
    "        \n",
    "        # è½¬æ¢æ©ç ä¸ºPyTorchå¼ é‡\n",
    "        # å½¢çŠ¶: (H, W, T) -> (T, H, W) å…¶ä¸­Tæ˜¯æ—¶é—´å¸§æ•°\n",
    "        mask = (\n",
    "            torch.tensor(mask_data)\n",
    "            .to(torch.float32)           # BCEWithLogitsLosséœ€è¦floatç±»å‹\n",
    "            .permute(2, 0, 1)           # (H, W, T) -> (T, H, W)\n",
    "        )\n",
    "        \n",
    "        return images, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c36fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½å™¨é…ç½®\n",
    "# ============================\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†å®ä¾‹\n",
    "print(\"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†...\")\n",
    "dataset_train = ContrailsAshDataset('train')\n",
    "dataset_validation = ContrailsAshDataset('validation')\n",
    "\n",
    "# ========================\n",
    "# æ•°æ®åŠ è½½å™¨ä¼˜åŒ–é…ç½®\n",
    "# ========================\n",
    "# é…ç½®æ•°æ®åŠ è½½å™¨å‚æ•°\n",
    "BATCH_SIZE = 16        # ç¨å¾®å¢åŠ æ‰¹å¤§å°ä»¥æé«˜GPUåˆ©ç”¨ç‡\n",
    "SHUFFLE_TRAIN = True   # è®­ç»ƒé›†éšæœºæ‰“ä¹±\n",
    "SHUFFLE_VAL = False    # éªŒè¯é›†ä¸æ‰“ä¹±ä»¥ä¿æŒä¸€è‡´æ€§\n",
    "\n",
    "print(\"âš™ï¸ ä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½å™¨é…ç½®:\")\n",
    "print(f\"   æ‰¹å¤§å°: {BATCH_SIZE}\")\n",
    "print(f\"   è®­ç»ƒé›†æ‰“ä¹±: {SHUFFLE_TRAIN}\")\n",
    "\n",
    "# ========================\n",
    "# æ•°æ®åŠ è½½å™¨æ€§èƒ½ä¼˜åŒ–æŠ€å·§\n",
    "# ========================\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE_TRAIN,\n",
    "    # num_workers=NUM_WORKERS,\n",
    "    drop_last=True,                 # ä¿è¯batchå¤§å°ä¸€è‡´\n",
    "    generator=torch.Generator().manual_seed(SEED)  # ç¡®ä¿å¯é‡ç°æ€§\n",
    ")\n",
    "\n",
    "# åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ\n",
    "data_loader_validation = DataLoader(\n",
    "    dataset_validation, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE_VAL,\n",
    "    # num_workers=NUM_WORKERS,\n",
    "    drop_last=False,                # éªŒè¯æ—¶ä¿ç•™æ‰€æœ‰æ•°æ®\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "print(\"âœ… ä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ!\")\n",
    "print(f\"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(data_loader_train)}\")\n",
    "print(f\"   éªŒè¯æ‰¹æ¬¡æ•°: {len(data_loader_validation)}\")\n",
    "print(f\"   æ¯epochè®­ç»ƒæ ·æœ¬: {len(data_loader_train) * BATCH_SIZE}\")\n",
    "print(f\"   æ¯epochéªŒè¯æ ·æœ¬: {len(dataset_validation)}\")\n",
    "\n",
    "# ========================\n",
    "# æ•°æ®åŠ è½½å™¨ä¼˜åŒ–è¯´æ˜\n",
    "# ========================\n",
    "print(\"\\nğŸ’¡ å·²åº”ç”¨çš„æ•°æ®åŠ è½½ä¼˜åŒ–:\")\n",
    "print(\"âœ… æ‰¹å¤§å°ä¼˜åŒ–: å¹³è¡¡å†…å­˜ä½¿ç”¨å’ŒGPUåˆ©ç”¨ç‡\")\n",
    "print(\"âœ… å¯é‡ç°æ€§: å›ºå®šéšæœºç§å­ç¡®ä¿ç»“æœä¸€è‡´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fca0f3",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# æ¨¡å‹è®­ç»ƒé…ç½®ä¸æ‰§è¡Œ - åŒ…å«è®­ç»ƒåŠ é€Ÿä¼˜åŒ–\n",
    "# ============================\n",
    "\n",
    "# è®­ç»ƒæ§åˆ¶å¼€å…³\n",
    "train = True  # True: ä»å¤´è®­ç»ƒæ–°æ¨¡å‹, False: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "if train:\n",
    "    print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...\")\n",
    "\n",
    "    # ========================\n",
    "    # è®­ç»ƒåŠ é€Ÿä¼˜åŒ–è®¾ç½®\n",
    "    # ========================\n",
    "    # 1. å¯ç”¨ç¼–è¯‘æ¨¡å¼åŠ é€Ÿ (PyTorch 2.0+)\n",
    "    torch.backends.cudnn.benchmark = True  # å¯¹å›ºå®šè¾“å…¥å¤§å°å¯ç”¨cuDNNä¼˜åŒ–\n",
    "\n",
    "    print(\"âœ… è®­ç»ƒåŠ é€Ÿä¼˜åŒ–å·²å¯ç”¨\")\n",
    "    print(\"   - cuDNN benchmark: True\")\n",
    "\n",
    "    # ========================\n",
    "    # æ¨¡å‹åˆå§‹åŒ–\n",
    "    # ========================\n",
    "    model = UNet() \n",
    "    model.to(mps_device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°MPSè®¾å¤‡\n",
    "    print(f\"âœ… æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {mps_device}\")\n",
    "\n",
    "    # ========================\n",
    "    # æŸå¤±å‡½æ•°é…ç½® - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬\n",
    "    # ========================\n",
    "    # ä½¿ç”¨å¸¦æƒé‡çš„äºŒå…ƒäº¤å‰ç†µæŸå¤± (Binary Cross Entropy with Logits Loss)\n",
    "    # å°†pos_weightç§»åŠ¨åˆ°è®¾å¤‡ä¸Šï¼Œé¿å…æ¯æ¬¡forwardæ—¶çš„æ•°æ®ä¼ è¾“\n",
    "    pos_weight = torch.tensor(100.0, device=mps_device, dtype=torch.float32)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    print(\"ğŸ“Š æŸå¤±å‡½æ•°: BCEWithLogitsLoss (pos_weight=100, è®¾å¤‡ä¼˜åŒ–)\")\n",
    "\n",
    "    # ========================\n",
    "    # ä¼˜åŒ–å™¨é…ç½® - æ€§èƒ½è°ƒä¼˜ç‰ˆæœ¬\n",
    "    # ========================\n",
    "    # Adamä¼˜åŒ–å™¨é…ç½®ï¼Œæ·»åŠ æ€§èƒ½ä¼˜åŒ–å‚æ•°\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.01,\n",
    "        weight_decay=1e-5,  # æ·»åŠ è½»å¾®L2æ­£åˆ™åŒ–ï¼Œæœ‰åŠ©äºæ”¶æ•›\n",
    "    )\n",
    "    print(\"ğŸ¯ ä¼˜åŒ–å™¨: Adam (æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬)\")\n",
    "\n",
    "    # ========================\n",
    "    # å­¦ä¹ ç‡è°ƒåº¦å™¨ -\n",
    "    # ========================\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.70)\n",
    "\n",
    "    # ========================\n",
    "    # è®­ç»ƒè¶…å‚æ•°\n",
    "    # ========================\n",
    "    num_epochs = 11  # è®­ç»ƒè½®æ¬¡\n",
    "\n",
    "    # ========================\n",
    "    # è®­ç»ƒæ‰§è¡Œ - å¢å¼ºç‰ˆè®­ç»ƒå™¨\n",
    "    # ========================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=criterion,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        device=mps_device,\n",
    "        use_amp=True,  # è®¾ç½®ä¸º True ä»¥å¯ç”¨æ··åˆç²¾åº¦\n",
    "    )\n",
    "\n",
    "    # æ‰§è¡Œä¼˜åŒ–è®­ç»ƒ\n",
    "    trainer.fit(\n",
    "        train_dataloader=data_loader_train,\n",
    "        test_dataloader=data_loader_validation,\n",
    "        epochs=num_epochs,\n",
    "        eval_every=1,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    # ========================\n",
    "    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "    # ========================\n",
    "    print(\"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...\")\n",
    "    model = UNet()\n",
    "\n",
    "    model_path = 'checkpoints/model_checkpoint_e11.pt'  # æ›´æ–°è·¯å¾„\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}\")\n",
    "        print(\"è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–å…ˆè¿è¡Œè®­ç»ƒä»£ç \")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(mps_device)\n",
    "    print(f\"ğŸ” æ¨¡å‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åŠ¨åˆ°: {mps_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1357309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if train:\n",
    "    df_data = pd.DataFrame({'Batch Losses': trainer.batch_losses})\n",
    "    sns.lineplot(data=df_data)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Batch Loss')\n",
    "    plt.show()\n",
    "\n",
    "    df_data = pd.DataFrame({'Batch Losses': trainer.epoch_losses})\n",
    "    sns.lineplot(data=df_data)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f65cca",
   "metadata": {},
   "source": [
    "# FIND OPTIMAL THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DiceThresholdTester:\n",
    "\n",
    "    def __init__(\n",
    "        self, model: nn.Module, data_loader: torch.utils.data.DataLoader\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.cumulative_mask_pred = []\n",
    "        self.cumulative_mask_true = []\n",
    "\n",
    "    def precalculate_prediction(self) -> None:\n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for images, mask_true in self.data_loader:\n",
    "            if mps_device:\n",
    "                images = images.to(mps_device)\n",
    "\n",
    "            mask_pred = sigmoid(model.forward(images))\n",
    "\n",
    "            self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n",
    "            self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n",
    "\n",
    "        self.cumulative_mask_pred = np.concatenate(\n",
    "            self.cumulative_mask_pred, axis=0\n",
    "        )\n",
    "        self.cumulative_mask_true = np.concatenate(\n",
    "            self.cumulative_mask_true, axis=0\n",
    "        )\n",
    "\n",
    "        self.cumulative_mask_pred = torch.flatten(\n",
    "            torch.from_numpy(self.cumulative_mask_pred)\n",
    "        )\n",
    "        self.cumulative_mask_true = torch.flatten(\n",
    "            torch.from_numpy(self.cumulative_mask_true)\n",
    "        )\n",
    "\n",
    "    def test_threshold(self, threshold: float) -> float:\n",
    "        _dice = Dice(use_sigmoid=False)\n",
    "        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n",
    "        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n",
    "        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n",
    "        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n",
    "        return _dice(self.cumulative_mask_true, after_threshold).item()\n",
    "\n",
    "# ============================\n",
    "dice_threshold_tester = DiceThresholdTester(model, data_loader_validation)\n",
    "dice_threshold_tester.precalculate_prediction()\n",
    "\n",
    "thresholds_to_test = [round(x * 0.01, 2) for x in range(101)]\n",
    "\n",
    "optim_threshold = 0.975\n",
    "best_dice_score = -1\n",
    "\n",
    "thresholds = []\n",
    "dice_scores = []\n",
    "\n",
    "for t in tqdm(thresholds_to_test, desc=\"Testing thresholds\"):\n",
    "    dice_score = dice_threshold_tester.test_threshold(t)\n",
    "    if dice_score > best_dice_score:\n",
    "        best_dice_score = dice_score\n",
    "        optim_threshold = t\n",
    "\n",
    "    thresholds.append(t)\n",
    "    dice_scores.append(dice_score)\n",
    "\n",
    "print(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\n",
    "df_threshold_data = pd.DataFrame(\n",
    "    {'Threshold': thresholds, 'Dice Score': dice_scores}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\n",
    "plt.axhline(y=best_dice_score, color='green')\n",
    "plt.axvline(x=optim_threshold, color='green')\n",
    "plt.text(\n",
    "    -0.02,\n",
    "    best_dice_score * 0.96,\n",
    "    f'{best_dice_score:.3f}',\n",
    "    va='center',\n",
    "    ha='left',\n",
    "    color='green',\n",
    ")\n",
    "plt.text(\n",
    "    optim_threshold - 0.01,\n",
    "    0.02,\n",
    "    f'{optim_threshold}',\n",
    "    va='center',\n",
    "    ha='right',\n",
    "    color='green',\n",
    ")\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Threshold vs Dice Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786abc26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
